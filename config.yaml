model:
  base_model_path:
    default: 'stabilityai/stable-diffusion-3.5-medium'
    type: str
    help: Base model for transfer learning
  data_type:
    default: 'bf16'
    type: str
    help: Data type for training

project_config:
  output_dir:
    default: '/home/xycoord/output-e2e'
    type: str
    help: Output directory

dataset:
  dataset_path:
    default: '/home/xycoord/models/Trans10k/'
    type: str
    help: Path to dataset
  dataset_name:
    default: 'trans10k'
    type: str
    help: Name of dataset
  val_difficulty:
    default: 'mix'
    type: str
    help: Difficulty level for validation dataset ['easy', 'hard', 'mix']
  test_difficulty:
    default: 'mix'
    type: str
    help: Difficulty level for test dataset ['easy', 'hard', 'mix']
  dataloader_num_workers:
    default: 4
    type: int
    help: Number of workers for dataloader

prompt_embeds:
  prompt_embeds_path:
    default: './precomputed_prompt_embeddings/'
    type: str
    help: Path to prompt embeds

training:
  resume_from_checkpoint:
    default: null 
    type: str
    help: Path to checkpoint to resume from
  epochs:
    default: 31
    type: int
    help: Number of epochs to train
  max_train_steps:
    default: null
    type: int
    help: Maximum number of training steps
  train_batch_size:
    default: 4
    type: int
    help: Batch size for training
  gradient_accumulation_steps:
    default: 2 #Change to 4 for 2 GPUs
    type: int
    help: Gradient accumulation steps
  save_checkpoint_steps:
    default: 200 
    type: int
    help: Save checkpoint every n steps
  save_checkpoint_epochs:
    default: 3
    type: int
    help: Save checkpoint every n epochs (overrides save_checkpoint_steps)
  checkpoint_offset_warmup:
    default: true
    type: bool
    help: Offset the checkpoint step by the number of warmup steps 
  val_steps:
    default: 157 
    type: int
    help: Validate every n steps

validation:
  load_checkpoint: 
    default: checkpoint-2347
    type: str
    help: Path to checkpoint to load for validation


learning_rate_scheduler:
  lr:
    default: 2.5e-5
    type: float
    help: Learning rate for training
  min_lr_ratio:
    default: 0.1
    type: float
    help: Ratio of the minimum lr to the initial lr 
  lr_scheduler:
    default: 'cosine'
    type: str
    help: Learning rate scheduler
  lr_cycles:
    default: 2.5
    type: float
    help: Number of cycles for learning rate scheduler
  lr_warmup_steps:
    default: null
    type: int
    help: Number of warmup steps for learning rate scheduler
  lr_warmup_epochs:
    default: 1
    type: int
    help: Number of warmup epochs for learning rate scheduler (overrides lr_warmup_steps)

optimizer:
  adam_beta1:
    default: 0.9
    type: float
    help: Beta1 for Adam optimizer
  adam_beta2:
    default: 0.999
    type: float
    help: Beta2 for Adam optimizer
  adam_weight_decay:
    default: 0.0
    type: float
    help: Weight decay for Adam optimizer
  adam_epsilon:
    default: 1e-15
    type: float
    help: Epsilon for Adam optimizer

noise:
  weighting_scheme:
    default: 'logit_normal'
    type: str
    help: Weighting scheme for noise ("sigma_sqrt", "logit_normal", "mode", "cosmap")
  logit_mean:
    default: 0.0
    type: float
    help: Mean for logit_normal weighting scheme
  logit_std:
    default: 1.0
    type: float
    help: Std for logit_normal weighting scheme
  mode_scale:
    default: 1.29
    type: float
    help: Scale of mode weighting scheme. Only effective when using the `'mode'` as the `weighting_scheme`.

gradient_clipping: # Ignore this if using DeepSpeed 
  max_grad_norm:
    default: 0.01 #1.0
    type: float
    help: Maximum gradient norm
      